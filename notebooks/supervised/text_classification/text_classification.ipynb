{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification (scikit-learn) with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this __Machine Learning Snippet__ we use scikit-learn (http://scikit-learn.org/) and ebooks from Project Gutenberg (https://www.gutenberg.org/) to create a text classifier, which can classify German, French, Dutch and English documents.\n",
    "\n",
    "We need one document per language and split the document into smaller chuncks to train the classifier.\n",
    "\n",
    "For our snippet we use the following ebooks:\n",
    "- _'A Christmas Carol'_ by Charles Dickens (English), https://www.gutenberg.org/ebooks/46\n",
    "- _'Der Weihnachtsabend'_ by Charles Dickens (German), https://www.gutenberg.org/ebooks/22465\n",
    "- _'Cantique de Noël'_ by Charles Dickens (French), https://www.gutenberg.org/ebooks/16021\n",
    "- _'Een Kerstlied in Proza'_ by Charles Dickens (Dutch), https://www.gutenberg.org/ebooks/28560\n",
    "\n",
    "\n",
    "__Note:__\n",
    "The ebooks are for the use of anyone anywhere at no cost and with\n",
    "almost no restrictions whatsoever.  You may copy it, give it away or\n",
    "re-use it under the terms of the Project Gutenberg License included\n",
    "with this eBook or online at www.gutenberg.org\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering data\n",
    "First let's extract the text without the header and footer from the ebooks and split the text by whitespace in tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:39.548488Z",
     "iopub.status.busy": "2021-03-09T08:39:39.547887Z",
     "iopub.status.idle": "2021-03-09T08:39:39.567081Z",
     "shell.execute_reply": "2021-03-09T08:39:39.566612Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens (german) 27218\n",
      "tokens (english) 28562\n",
      "tokens (french) 32758\n",
      "tokens (dutch) 31506\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "\n",
    "'''\n",
    "with urllib.request.urlopen('http://www.gutenberg.org/cache/epub/22465/pg22465.txt') as response:\n",
    "   txt_german = response.read().decode('utf-8') \n",
    "\n",
    "with urllib.request.urlopen('https://www.gutenberg.org/files/46/46-0.txt') as response:\n",
    "   txt_english = response.read().decode('utf-8') \n",
    "\n",
    "with urllib.request.urlopen('http://www.gutenberg.org/cache/epub/16021/pg16021.txt') as response:\n",
    "   txt_french = response.read().decode('utf-8') \n",
    "\n",
    "with urllib.request.urlopen('http://www.gutenberg.org/cache/epub/28560/pg28560.txt') as response:\n",
    "   txt_dutch = response.read().decode('utf-8') \n",
    "'''\n",
    "\n",
    "with open('data/pg22465.txt', 'r') as reader:\n",
    "    txt_german = reader.read()\n",
    "\n",
    "with open('data/46-0.txt', 'r') as reader:\n",
    "    txt_english = reader.read()\n",
    "\n",
    "with open('data/pg16021.txt', 'r') as reader:\n",
    "    txt_french = reader.read()\n",
    "\n",
    "with open('data/pg28560.txt', 'r') as reader:\n",
    "    txt_dutch = reader.read()\n",
    "\n",
    "\n",
    "def get_markers(txt, begin_pattern, end_pattern):\n",
    "    iter = re.finditer(begin_pattern, txt)\n",
    "    index_headers = [m.start(0) for m in iter]\n",
    "    \n",
    "    iter = re.finditer(end_pattern, txt)\n",
    "    index_footers = [m.start(0) for m in iter]    \n",
    "    \n",
    "    # return first match\n",
    "    return index_headers[0] + len(begin_pattern.replace('\\\\','')), index_footers[0]\n",
    "\n",
    "def extract_text_tokens(txt, \n",
    "                        begin_pattern='\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK', \n",
    "                        end_pattern='\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK'):\n",
    "    header, footer = get_markers(txt, begin_pattern, end_pattern)\n",
    "    return txt[header: footer].split()\n",
    "\n",
    "\n",
    "tokens_german = extract_text_tokens(txt_german)\n",
    "tokens_english = extract_text_tokens(txt_english)\n",
    "tokens_french = extract_text_tokens(txt_french)\n",
    "tokens_dutch = extract_text_tokens(txt_dutch)\n",
    "\n",
    "print('tokens (german)', len(tokens_german))\n",
    "print('tokens (english)', len(tokens_english))\n",
    "print('tokens (french)', len(tokens_french))\n",
    "print('tokens (dutch)', len(tokens_dutch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Next we do some data cleaning. This means we remove special characters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:39.607993Z",
     "iopub.status.busy": "2021-03-09T08:39:39.580107Z",
     "iopub.status.idle": "2021-03-09T08:39:39.815444Z",
     "shell.execute_reply": "2021-03-09T08:39:39.815027Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned tokens (german) 27181\n",
      "cleaned tokens (french) 31995\n",
      "cleaned tokens (dutch) 31405\n",
      "cleaned tokens (english) 27527\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_chars(x):\n",
    "    \n",
    "    # remove special characters\n",
    "    chars = ['_', '(', ')', '*', '\"', '[', ']', '?', '!', ',', '.', '»', '«', ':', ';']\n",
    "    for c in chars:\n",
    "        x = x.replace(c, '')\n",
    "    \n",
    "    # remove numbers\n",
    "    x = re.sub('\\d', '', x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def clean_data(featurs): \n",
    "    # strip, remove sepcial characters and numbers\n",
    "    tokens = [remove_special_chars(x.strip()) for x in featurs]\n",
    "    \n",
    "    cleaned = []\n",
    "    \n",
    "    # only use words with length > 1\n",
    "    for t in tokens:\n",
    "        if len(t) > 1:\n",
    "            cleaned.append(t)\n",
    "            \n",
    "    return cleaned\n",
    "\n",
    "cleaned_tokens_english = clean_data(tokens_english)\n",
    "cleaned_tokens_german = clean_data(tokens_german)\n",
    "cleaned_tokens_french = clean_data(tokens_french)\n",
    "cleaned_tokens_dutch = clean_data(tokens_dutch)\n",
    "\n",
    "\n",
    "print('cleaned tokens (german)', len(cleaned_tokens_german))\n",
    "print('cleaned tokens (french)', len(cleaned_tokens_french))\n",
    "print('cleaned tokens (dutch)', len(cleaned_tokens_dutch))\n",
    "print('cleaned tokens (english)', len(cleaned_tokens_english))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create for every language 1300 text samples with 20 tokens (words). These samples will later be \n",
    "used to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:39.820356Z",
     "iopub.status.busy": "2021-03-09T08:39:39.819928Z",
     "iopub.status.idle": "2021-03-09T08:39:40.255170Z",
     "shell.execute_reply": "2021-03-09T08:39:40.254800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples (german) 1300\n",
      "samples (french) 1300\n",
      "samples (dutch) 1300\n",
      "samples (english) 1300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "max_tokens = 20\n",
    "max_samples = 1300\n",
    "\n",
    "\n",
    "def create_text_sample(x):\n",
    "    \n",
    "    data = []\n",
    "    text = []\n",
    "    for i, f in enumerate(x):\n",
    "        text.append(f)\n",
    "        if i % max_tokens == 0 and i != 0:\n",
    "            data.append(' '.join(text))\n",
    "            text = []\n",
    "    return data\n",
    "    \n",
    "\n",
    "sample_german = resample(create_text_sample(cleaned_tokens_german), replace=False, n_samples=max_samples)\n",
    "sample_french = resample(create_text_sample(cleaned_tokens_french), replace=False, n_samples=max_samples)\n",
    "sample_dutch = resample(create_text_sample(cleaned_tokens_dutch), replace=False, n_samples=max_samples)\n",
    "sample_english = resample(create_text_sample(cleaned_tokens_english), replace=False, n_samples=max_samples)\n",
    "\n",
    "print('samples (german)', len(sample_german))\n",
    "print('samples (french)', len(sample_french))\n",
    "print('samples (dutch)', len(sample_dutch))\n",
    "print('samples (english)', len(sample_english))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text sample looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:40.258861Z",
     "iopub.status.busy": "2021-03-09T08:39:40.258283Z",
     "iopub.status.idle": "2021-03-09T08:39:40.260721Z",
     "shell.execute_reply": "2021-03-09T08:39:40.261033Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sample:\n",
      "------------------\n",
      "to Scrooge or to any one whom he could see but it produced an immediate effect For again Scrooge saw\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print('English sample:\\n------------------')\n",
    "print(sample_english[0])\n",
    "print('------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a model\n",
    "As classifier we use the MultinomialNB classifier with the TfidfVectorizer. \n",
    "\n",
    "First we create the data structure which we will use to train the model. \n",
    "\n",
    "```\n",
    "{\n",
    "    samples: {\n",
    "        text:[], \n",
    "        target: []\n",
    "    }\n",
    "    labels: [] \n",
    "}\n",
    " \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:40.273009Z",
     "iopub.status.busy": "2021-03-09T08:39:40.268496Z",
     "iopub.status.idle": "2021-03-09T08:39:40.276105Z",
     "shell.execute_reply": "2021-03-09T08:39:40.275626Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  ['de', 'en', 'fr', 'nl']\n",
      "target (labels encoded):  {0, 1, 2, 3}\n",
      "samples:  5200\n"
     ]
    }
   ],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "def create_data_structure(**kwargs):\n",
    "    data = dotdict({'labels':[]})\n",
    "    data.samples = dotdict({'text': [], 'target': []})\n",
    "    \n",
    "    label = 0\n",
    "    for name, value in kwargs.items():\n",
    "        data.labels.append(name)\n",
    "        for i in value:\n",
    "            data.samples.text.append(i)\n",
    "            data.samples.target.append(label)\n",
    "        label += 1\n",
    "            \n",
    "    \n",
    "    return data\n",
    "\n",
    "data = create_data_structure(de = sample_german, en = sample_english, \n",
    "                             fr = sample_french, nl = sample_dutch)\n",
    "\n",
    "print('labels: ', data.labels)\n",
    "print('target (labels encoded): ', set(data.samples.target))\n",
    "print('samples: ', len(data.samples.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "It's importan that we shuffle and split the data into training (70%) and test set (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:40.279529Z",
     "iopub.status.busy": "2021-03-09T08:39:40.279119Z",
     "iopub.status.idle": "2021-03-09T08:39:40.334772Z",
     "shell.execute_reply": "2021-03-09T08:39:40.334278Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (x, y):  3120 3120\n",
      "test size (x, y):  2080 2080\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.samples.text, data.samples.target, test_size=0.40)\n",
    "\n",
    "print('train size (x, y): ', len(x_train),  len(y_train))\n",
    "print('test size (x, y): ', len(x_test), len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We connect all our parts (classifier, etc.) to our _Machine Learning Pipeline_. So it’s easier and faster to go trough all processing steps to build a model.\n",
    "\n",
    "The TfidfVectorizer will use the the word analyzer, min document frequency of 10  and convert the text to lowercase. I know we already did a lowercase conversion in the previous step. We also provide some stop words which should be ignored in our model. \n",
    "\n",
    "The MultinomialNB classifier wil use the default alpha value 1.0.\n",
    "\n",
    "Here you can play around with the settings. In the next section you see how to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:40.338660Z",
     "iopub.status.busy": "2021-03-09T08:39:40.338185Z",
     "iopub.status.idle": "2021-03-09T08:39:40.345794Z",
     "shell.execute_reply": "2021-03-09T08:39:40.345299Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "stopwords = ['scrooge', 'scrooges', 'bob']\n",
    "\n",
    "pipeline = Pipeline([('vect', TfidfVectorizer(analyzer='word', \n",
    "                            min_df=10, lowercase=True, stop_words=stopwords)),\n",
    "                      ('clf', MultinomialNB(alpha=1.0))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "In this step we will evaluate the performance of our classifier. So we do the following evaluation:\n",
    "- Evaluate the model with k-fold on the training set\n",
    "- Evaluate the final model with the test set\n",
    "\n",
    "First let's evaluate our model with a k-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:40.350327Z",
     "iopub.status.busy": "2021-03-09T08:39:40.349913Z",
     "iopub.status.idle": "2021-03-09T08:39:40.999696Z",
     "shell.execute_reply": "2021-03-09T08:39:40.999304Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_weighted\n",
      "scores: [1.         1.         1.         0.99839749 1.        ]\n",
      "f1_weighted: 0.999679 (+/- 0.0013)\n",
      "\n",
      "accuracy\n",
      "scores: [1.         1.         1.         0.99839744 1.        ]\n",
      "accuracy: 0.999679 (+/- 0.0013)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection\n",
    "\n",
    "folds = 5\n",
    "\n",
    "for scoring in ['f1_weighted', 'accuracy']:\n",
    "\n",
    "    scores = model_selection.cross_val_score(pipeline, X=x_train, y=y_train, \n",
    "                                         cv=folds, scoring=scoring)\n",
    "    print(scoring)\n",
    "    print('scores: %s' % scores )\n",
    "    print(scoring + ': %0.6f (+/- %0.4f)' % (scores.mean(), scores.std() * 2))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build the model and evaluate the result against our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:41.014173Z",
     "iopub.status.busy": "2021-03-09T08:39:41.008055Z",
     "iopub.status.idle": "2021-03-09T08:39:41.105413Z",
     "shell.execute_reply": "2021-03-09T08:39:41.104932Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000       518\n",
      "           1     0.9981    1.0000    0.9991       535\n",
      "           2     1.0000    1.0000    1.0000       519\n",
      "           3     1.0000    0.9980    0.9990       508\n",
      "\n",
      "    accuracy                         0.9995      2080\n",
      "   macro avg     0.9995    0.9995    0.9995      2080\n",
      "weighted avg     0.9995    0.9995    0.9995      2080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "text_clf = pipeline.fit(x_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(x_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the features of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are the most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:41.110549Z",
     "iopub.status.busy": "2021-03-09T08:39:41.109766Z",
     "iopub.status.idle": "2021-03-09T08:39:41.113564Z",
     "shell.execute_reply": "2021-03-09T08:39:41.113875Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de: das ein ich es zu sie er die der und\n",
      "en: was that in his he it of to and the\n",
      "fr: se une que les un il la et le de\n",
      "nl: op te dat hij zijn van een het de en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mru/.local/share/virtualenvs/machine-learning-snippets-mLikUPnf/lib/python3.8/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# show most informative features\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "\n",
    "show_top10(text_clf.named_steps['clf'], text_clf.named_steps['vect'], data.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which and how many features our model has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:41.117794Z",
     "iopub.status.busy": "2021-03-09T08:39:41.117102Z",
     "iopub.status.idle": "2021-03-09T08:39:41.120514Z",
     "shell.execute_reply": "2021-03-09T08:39:41.120072Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 829\n",
      "first features: ['aan' 'aber' 'about' 'af' 'affaires' 'after' 'again' 'ah' 'ai' 'air']\n",
      "last features: ['zum' 'zurück' 'zwei' 'écria' 'étaient' 'était' 'été' 'één' 'être' 'über']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.asarray(text_clf.named_steps['vect'].get_feature_names())\n",
    "\n",
    "print('number of features: %d' % len(feature_names))\n",
    "print('first features: %s'% feature_names[0:10])\n",
    "print('last features: %s' % feature_names[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New data\n",
    "Let's try out the classifier with the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-03-09T08:39:41.125887Z",
     "iopub.status.busy": "2021-03-09T08:39:41.125467Z",
     "iopub.status.idle": "2021-03-09T08:39:41.130981Z",
     "shell.execute_reply": "2021-03-09T08:39:41.130496Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo mein Name ist Hugo.  -->  de , prob: 0.8455476679547136\n",
      "Hi my name is Hugo.  -->  en , prob: 0.8538272747783979\n",
      "Bonjour mon nom est Hugo.  -->  fr , prob: 0.9431017418169352\n",
      "Hallo mijn naam is Hugo.  -->  nl , prob: 0.8268202913919833\n",
      "Eins, zwei und drei.  -->  de , prob: 0.912783653860269\n",
      "One, two and three.  -->  en , prob: 0.9701529290991253\n",
      "Un, deux et trois.  -->  fr , prob: 0.9906749899890257\n",
      "Een, twee en drie.  -->  nl , prob: 0.9681750228498841\n"
     ]
    }
   ],
   "source": [
    "new_data = ['Hallo mein Name ist Hugo.', \n",
    "            'Hi my name is Hugo.', \n",
    "            'Bonjour mon nom est Hugo.',\n",
    "            'Hallo mijn naam is Hugo.',\n",
    "            'Eins, zwei und drei.',\n",
    "            'One, two and three.',\n",
    "            'Un, deux et trois.',\n",
    "            'Een, twee en drie.'\n",
    "           ]\n",
    "\n",
    "predicted = text_clf.predict(new_data)\n",
    "probs = text_clf.predict_proba(new_data)\n",
    "for i, p in enumerate(predicted):\n",
    "    print(new_data[i], ' --> ', data.labels[p], ', prob:' , probs[i][p])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
