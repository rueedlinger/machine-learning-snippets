{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification (scikit-learn) with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this __Machine Learning Snippet__ we use scikit-learn (http://scikit-learn.org/) and ebooks from Project Gutenberg (https://www.gutenberg.org/) to create a text classifier, which can classify German, French, Dutch and English texts.\n",
    "\n",
    "For our snippet we use the following ebooks:\n",
    "- _'A Christmas Carol'_ by Charles Dickens (English), https://www.gutenberg.org/ebooks/46\n",
    "- _'Der Weihnachtsabend'_ by Charles Dickens (German), https://www.gutenberg.org/ebooks/22465\n",
    "- _'Cantique de Noël'_ by Charles Dickens (French), https://www.gutenberg.org/ebooks/16021\n",
    "- _'Een Kerstlied in Proza'_ by Charles Dickens (Dutch), https://www.gutenberg.org/ebooks/28560\n",
    "\n",
    "\n",
    "__Note:__\n",
    "The eBooks are for the use of anyone anywhere at no cost and with\n",
    "almost no restrictions whatsoever.  You may copy it, give it away or\n",
    "re-use it under the terms of the Project Gutenberg License included\n",
    "with this eBook or online at www.gutenberg.org\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We prepare the English, French and German text. The goal is to cut off the header and footer from the ebooks. Then we do some text cleaning:\n",
    "- Convert to lowercase\n",
    "- Tokenize  the text by space\n",
    "- Remove special chars (new lines, etc.)\n",
    "- Remove numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's extract the text tokens (words) from the text. Convert text to lowercase and  remove the header and the footer from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "txt_german = open('data/pg22465.txt', 'r').read()\n",
    "txt_english = open('data/pg46.txt', 'r').read()\n",
    "txt_french = open('data/pg16021.txt', 'r').read()\n",
    "txt_dutch = open('data/pg28560.txt', 'r').read()\n",
    "\n",
    "\n",
    "\n",
    "def get_markers(txt, pattern='\\*\\*\\*'):\n",
    "    iter = re.finditer(pattern, txt)\n",
    "    indices = [m.start(0) for m in iter]\n",
    "    return indices\n",
    "\n",
    "def extract_text_tokens(txt):\n",
    "    indices = get_markers(txt)\n",
    "    header = indices[1]\n",
    "    footer = indices[2]\n",
    "    \n",
    "    return txt[header: footer].lower().strip().split()\n",
    "\n",
    "\n",
    "feat_german = extract_text_tokens(txt_german)\n",
    "feat_english = extract_text_tokens(txt_english)\n",
    "feat_french = extract_text_tokens(txt_french)\n",
    "feat_dutch = extract_text_tokens(txt_dutch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create text tokens and remove the special characters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens (german) 27216\n",
      "tokens (french) 32755\n",
      "tokens (dutch) 31502\n",
      "tokens (english) 28559\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_chars(x):\n",
    "    \n",
    "    chars = ['_', '(', ')', '*', '\"', '[', ']', '?', '!', ',', '.', '»', '«', ':', ';']\n",
    "    for c in chars:\n",
    "        x = x.replace(c, '')\n",
    "    \n",
    "    # remove numbers\n",
    "    x = re.sub('\\d', '', x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "tokens_english = [remove_special_chars(x) for x in feat_english]\n",
    "tokens_german = [remove_special_chars(x) for x in feat_german]\n",
    "tokens_french = [remove_special_chars(x) for x in feat_french]\n",
    "tokens_dutch = [remove_special_chars(x) for x in feat_dutch]\n",
    "\n",
    "\n",
    "print('tokens (german)', len(tokens_german))\n",
    "print('tokens (french)', len(tokens_french))\n",
    "print('tokens (dutch)', len(tokens_dutch))\n",
    "print('tokens (english)', len(tokens_english))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Now we create text samples from 20 tokens (words). The toknes from the samples will be used to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples (german) 1360\n",
      "samples (french) 1637\n",
      "samples (dutch) 1575\n",
      "samples (english) 1427\n"
     ]
    }
   ],
   "source": [
    "def create_text_sample(x):\n",
    "    max_tokens = 20\n",
    "    data = []\n",
    "    text = []\n",
    "    for i, f in enumerate(x):\n",
    "        text.append(f)\n",
    "        if i % max_tokens == 0 and i != 0:\n",
    "            data.append(' '.join(text))\n",
    "            text = []\n",
    "    return data\n",
    "    \n",
    "\n",
    "sample_german = create_text_sample(tokens_german)\n",
    "sample_french = create_text_sample(tokens_french)\n",
    "sample_dutch = create_text_sample(tokens_dutch)\n",
    "sample_english = create_text_sample(tokens_english)\n",
    "\n",
    "print('samples (german)', len(sample_german))\n",
    "print('samples (french)', len(sample_french))\n",
    "print('samples (dutch)', len(sample_dutch))\n",
    "print('samples (english)', len(sample_english))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text sample looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sample:\n",
      "------------------\n",
      "very night we have no doubt his liberality is well represented by his surviving partner said the gentleman presenting his\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print('English sample:\\n------------------')\n",
    "print(sample_english[100])\n",
    "print('------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "As classifier we use the MultinomialNB classifier with the TfidfVectorizer. The TfidfVectorizer will use the the word analyzer and convert the text to lowercase. We need to do the following steps:\n",
    "- Create the data structure for the classifier\n",
    "- Split the data into test and training set\n",
    "- Create the _Machine Learning Pipeline_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target names:  ['en', 'nl', 'fr', 'de']\n",
      "number of observations:  5999\n"
     ]
    }
   ],
   "source": [
    "import argparse as ap\n",
    "\n",
    "def create_data_structure(**kwargs):\n",
    "    samples = {'data': [], 'target': [], 'target_names':[]}\n",
    "    label = 0\n",
    "    for name, value in kwargs.items():\n",
    "        samples['target_names'].append(name)\n",
    "        for i in value:\n",
    "            samples['data'].append(i)\n",
    "            samples['target'].append(label)\n",
    "        label += 1\n",
    "            \n",
    "    \n",
    "    return ap.Namespace(**samples)\n",
    "\n",
    "data = create_data_structure(de = sample_german, en = sample_english, \n",
    "                             fr = sample_french, nl = sample_dutch)\n",
    "\n",
    "\n",
    "\n",
    "print('target names: ', data.target_names)\n",
    "print('number of observations: ', len(data.data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training (70%) and test set (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (x, y):  4199 4199\n",
      "test size (x, y):  1800 1800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.30, random_state=np.random.randint(low=0, high=10000))\n",
    "\n",
    "print('train size (x, y): ', len(x_train),  len(y_train))\n",
    "print('test size (x, y): ', len(x_test), len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the following _Machine Learning Pipeline_ (model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "pipeline = Pipeline([('vect', TfidfVectorizer(analyzer='word', min_df=1, lowercase=True)),\n",
    "                      ('clf', MultinomialNB()),])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "In this step we want to evaluate the performance of our classifier. So we do the following evaluation:\n",
    "- Evaluate the model with k-fold on the training set\n",
    "- Evaluate the final model with the holdout set (test set)\n",
    "\n",
    "Let's evaluate our model with k-fold against our training set. In this step we can tune the model and settings with the output from the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.999523 (+/- 0.0013)\n"
     ]
    }
   ],
   "source": [
    "folds = 6\n",
    "scores = model_selection.cross_val_score(pipeline, X=x_train, y=y_train, \n",
    "                                         cv=folds, scoring='accuracy')\n",
    "\n",
    "print('Accuracy: %0.6f (+/- %0.4f)' % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         en     0.9980    1.0000    0.9990       992\n",
      "         nl     1.0000    1.0000    1.0000      1129\n",
      "         fr     1.0000    0.9991    0.9996      1124\n",
      "         de     1.0000    0.9990    0.9995       954\n",
      "\n",
      "avg / total     0.9995    0.9995    0.9995      4199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model_selection.cross_val_predict(pipeline, X=x_train, y=y_train, cv=folds)\n",
    "print(metrics.classification_report(y_train, predicted, \n",
    "                                    target_names=data.target_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our classifier with the holdout set (test set) against the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         en     1.0000    1.0000    1.0000       435\n",
      "         nl     1.0000    1.0000    1.0000       446\n",
      "         fr     1.0000    1.0000    1.0000       513\n",
      "         de     1.0000    1.0000    1.0000       406\n",
      "\n",
      "avg / total     1.0000    1.0000    1.0000      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = pipeline.fit(x_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(x_test)\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted, target_names=data.target_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New data\n",
    "Let's try out the classifier with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo mein Name ist Hugo.  -->  de , prob: 0.639591312356\n",
      "Hi my name is Hugo.  -->  en , prob: 0.699118050854\n",
      "Bonjour mon nom est Hugo.  -->  fr , prob: 0.827110536445\n",
      "Hallo mijn naam is Hugo.  -->  nl , prob: 0.76476440211\n",
      "Eins, zwei und drei.  -->  de , prob: 0.910709776629\n",
      "One, two and three.  -->  en , prob: 0.964875844812\n",
      "Un, deux et trois.  -->  fr , prob: 0.97930674257\n",
      "Een, twee en drie.  -->  nl , prob: 0.956671833056\n"
     ]
    }
   ],
   "source": [
    "new_data = ['Hallo mein Name ist Hugo.', \n",
    "            'Hi my name is Hugo.', \n",
    "            'Bonjour mon nom est Hugo.',\n",
    "            'Hallo mijn naam is Hugo.',\n",
    "            'Eins, zwei und drei.',\n",
    "            'One, two and three.',\n",
    "            'Un, deux et trois.',\n",
    "            'Een, twee en drie.'\n",
    "           ]\n",
    "\n",
    "predicted = text_clf.predict(new_data)\n",
    "probs = text_clf.predict_proba(new_data)\n",
    "for i, p in enumerate(predicted):\n",
    "    print(new_data[i], ' --> ', data.target_names[p], ', prob:' , max(probs[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are the most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: was in that his he it of to and the\n",
      "nl: ik te dat van zijn hij de het een en\n",
      "fr: qu une que les un la il et le de\n",
      "de: ein das ich zu es sie er die der und\n"
     ]
    }
   ],
   "source": [
    "# show most informative features\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "\n",
    "show_top10(text_clf.named_steps['clf'], text_clf.named_steps['vect'], data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
