{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification (scikit-learn) with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this __Machine Learning Snippet__ we use scikit-learn (http://scikit-learn.org/) and ebooks from Project Gutenberg (https://www.gutenberg.org/) to create a text classifier, which can classify German, French, Dutch and English documents.\n",
    "\n",
    "We need one document per language and split the document into smaller chuncks to train the classifier.\n",
    "\n",
    "For our snippet we use the following ebooks:\n",
    "- _'A Christmas Carol'_ by Charles Dickens (English), https://www.gutenberg.org/ebooks/46\n",
    "- _'Der Weihnachtsabend'_ by Charles Dickens (German), https://www.gutenberg.org/ebooks/22465\n",
    "- _'Cantique de Noël'_ by Charles Dickens (French), https://www.gutenberg.org/ebooks/16021\n",
    "- _'Een Kerstlied in Proza'_ by Charles Dickens (Dutch), https://www.gutenberg.org/ebooks/28560\n",
    "\n",
    "\n",
    "__Note:__\n",
    "The ebooks are for the use of anyone anywhere at no cost and with\n",
    "almost no restrictions whatsoever.  You may copy it, give it away or\n",
    "re-use it under the terms of the Project Gutenberg License included\n",
    "with this eBook or online at www.gutenberg.org\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We have to prepare the English, French, Dutch and German text before we can beginn. The goal is to cut off the header and footer from the ebooks. Then we do some text cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's extract the text without the header and footer from the ebooks, convert to lowercase and split the text by whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "txt_german = open('data/pg22465.txt', 'r').read()\n",
    "txt_english = open('data/pg46.txt', 'r').read()\n",
    "txt_french = open('data/pg16021.txt', 'r').read()\n",
    "txt_dutch = open('data/pg28560.txt', 'r').read()\n",
    "\n",
    "\n",
    "\n",
    "def get_markers(txt, pattern='\\*\\*\\*'):\n",
    "    iter = re.finditer(pattern, txt)\n",
    "    indices = [m.start(0) for m in iter]\n",
    "    return indices\n",
    "\n",
    "def extract_text_tokens(txt):\n",
    "    indices = get_markers(txt)\n",
    "    header = indices[1]\n",
    "    footer = indices[2]\n",
    "    \n",
    "    return txt[header: footer].lower().strip().split()\n",
    "\n",
    "\n",
    "feat_german = extract_text_tokens(txt_german)\n",
    "feat_english = extract_text_tokens(txt_english)\n",
    "feat_french = extract_text_tokens(txt_french)\n",
    "feat_dutch = extract_text_tokens(txt_dutch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create text tokens and remove special characters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens (german) 27216\n",
      "tokens (french) 32755\n",
      "tokens (dutch) 31502\n",
      "tokens (english) 28559\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_chars(x):\n",
    "    \n",
    "    chars = ['_', '(', ')', '*', '\"', '[', ']', '?', '!', ',', '.', '»', '«', ':', ';']\n",
    "    for c in chars:\n",
    "        x = x.replace(c, '')\n",
    "    \n",
    "    # remove numbers\n",
    "    x = re.sub('\\d', '', x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "tokens_english = [remove_special_chars(x) for x in feat_english]\n",
    "tokens_german = [remove_special_chars(x) for x in feat_german]\n",
    "tokens_french = [remove_special_chars(x) for x in feat_french]\n",
    "tokens_dutch = [remove_special_chars(x) for x in feat_dutch]\n",
    "\n",
    "\n",
    "print('tokens (german)', len(tokens_german))\n",
    "print('tokens (french)', len(tokens_french))\n",
    "print('tokens (dutch)', len(tokens_dutch))\n",
    "print('tokens (english)', len(tokens_english))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create samples\n",
    "Now we create text samples from 20 tokens (words). The tokens from the samples will be later used to train the classifier. We will only use 1300 samples from every langauge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples (german) 1300\n",
      "samples (french) 1300\n",
      "samples (dutch) 1300\n",
      "samples (english) 1300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "max_samples = 1300\n",
    "\n",
    "def create_text_sample(x):\n",
    "    max_tokens = 20\n",
    "    data = []\n",
    "    text = []\n",
    "    for i, f in enumerate(x):\n",
    "        text.append(f)\n",
    "        if i % max_tokens == 0 and i != 0:\n",
    "            data.append(' '.join(text))\n",
    "            text = []\n",
    "    return data\n",
    "    \n",
    "\n",
    "sample_german = resample(create_text_sample(tokens_german), replace=False, n_samples=max_samples)\n",
    "sample_french = resample(create_text_sample(tokens_french), replace=False, n_samples=max_samples)\n",
    "sample_dutch = resample(create_text_sample(tokens_dutch), replace=False, n_samples=max_samples)\n",
    "sample_english = resample(create_text_sample(tokens_english), replace=False, n_samples=max_samples)\n",
    "\n",
    "print('samples (german)', len(sample_german))\n",
    "print('samples (french)', len(sample_french))\n",
    "print('samples (dutch)', len(sample_dutch))\n",
    "print('samples (english)', len(sample_english))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text sample looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sample:\n",
      "------------------\n",
      "spirit said scrooge after a moment's thought i wonder you of all the beings in the many worlds about us\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print('English sample:\\n------------------')\n",
    "print(sample_english[100])\n",
    "print('------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "As classifier we use the MultinomialNB classifier with the TfidfVectorizer. \n",
    "\n",
    "First we create the data structure which we used to train the model. The data structure will have list with the data (X), target (y) and target names.\n",
    "\n",
    "```\n",
    "{data: [], target: [], target_names: [] } \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target names:  ['fr', 'en', 'de', 'nl']\n",
      "number of observations:  5200\n"
     ]
    }
   ],
   "source": [
    "import argparse as ap\n",
    "\n",
    "def create_data_structure(**kwargs):\n",
    "    samples = {'data': [], 'target': [], 'target_names':[]}\n",
    "    label = 0\n",
    "    for name, value in kwargs.items():\n",
    "        samples['target_names'].append(name)\n",
    "        for i in value:\n",
    "            samples['data'].append(i)\n",
    "            samples['target'].append(label)\n",
    "        label += 1\n",
    "            \n",
    "    \n",
    "    return ap.Namespace(**samples)\n",
    "\n",
    "data = create_data_structure(de = sample_german, en = sample_english, \n",
    "                             fr = sample_french, nl = sample_dutch)\n",
    "\n",
    "\n",
    "\n",
    "print('target names: ', data.target_names)\n",
    "print('number of observations: ', len(data.data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's importan that we shuffle and split the data into training (70%) and test set (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (x, y):  3640 3640\n",
      "test size (x, y):  1560 1560\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.30)\n",
    "\n",
    "print('train size (x, y): ', len(x_train),  len(y_train))\n",
    "print('test size (x, y): ', len(x_test), len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We connect all our parts (classifier, etc.) to our _Machine Learning Pipeline_. So it’s easier and faster to go trough all processing steps to build a model.\n",
    "\n",
    "The TfidfVectorizer will use the the word analyzer, min document frequency of 10  and convert the text to lowercase. I know we already did a lowercase conversion in the previous step. We also provide some stop words which should be ignored in our model. \n",
    "\n",
    "The MultinomialNB classifier wil use the default alpha value 1.0.\n",
    "\n",
    "Here you can play around with the settings. In the next section you see how to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "stopwords = ['scrooge', 'scrooges', 'bob']\n",
    "\n",
    "pipeline = Pipeline([('vect', TfidfVectorizer(analyzer='word', min_df=10, lowercase=True, stop_words=stopwords)),\n",
    "                      ('clf', MultinomialNB(alpha=1.0))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "In this step we want to evaluate the performance of our classifier. So we do the following evaluation:\n",
    "- Evaluate the model with k-fold on the training set\n",
    "- Evaluate the final model with the test set\n",
    "\n",
    "Let's evaluate our model with k-fold against our training set. In this step we can tune the model and settings with the output from the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [ 0.99725275  1.          1.          1.          1.        ]\n",
      "accuracy: 0.999451 (+/- 0.0022)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = 5\n",
    "kf = KFold(n_splits=folds)\n",
    "\n",
    "'''\n",
    "scores = model_selection.cross_val_score(pipeline, X=x_train, y=y_train, \n",
    "                                         cv=folds, scoring='f1_weighted')\n",
    "'''\n",
    "\n",
    "scores = model_selection.cross_val_score(pipeline, X=x_train, y=y_train, \n",
    "                                         cv=kf, scoring='accuracy')\n",
    "\n",
    "print('scores: %s' % scores )\n",
    "\n",
    "print('accuracy: %0.6f (+/- %0.4f)' % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         fr     1.0000    0.9989    0.9995       930\n",
      "         en     0.9978    1.0000    0.9989       915\n",
      "         de     1.0000    0.9989    0.9994       895\n",
      "         nl     1.0000    1.0000    1.0000       900\n",
      "\n",
      "avg / total     0.9995    0.9995    0.9995      3640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predicted = model_selection.cross_val_predict(pipeline, X=x_train, y=y_train, cv=folds)\n",
    "print(metrics.classification_report(y_train, predicted, \n",
    "                                    target_names=data.target_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the final model with the fold, which had the best score. We will not use the whole training set because we might overfit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def select_best_kfold(x, y, kf, scores):\n",
    "\n",
    "    splitts = list(kf.split(x))\n",
    "    \n",
    "    score_index = np.argmax(scores == max(scores))\n",
    "    train_index = splitts[score_index][0]\n",
    "    \n",
    "    return np.array(x)[train_index], np.array(y)[train_index]\n",
    "\n",
    "x_final, y_final = select_best_kfold(x_train, y_train, kf, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build the model and evaluate the result against our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         fr     1.0000    1.0000    1.0000       370\n",
      "         en     1.0000    1.0000    1.0000       385\n",
      "         de     1.0000    1.0000    1.0000       405\n",
      "         nl     1.0000    1.0000    1.0000       400\n",
      "\n",
      "avg / total     1.0000    1.0000    1.0000      1560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = pipeline.fit(x_final, y_final)\n",
    "\n",
    "predicted = text_clf.predict(x_test)\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted, target_names=data.target_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the features of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are the most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr: qu que une les un il la et le de\n",
      "en: in was that his he it of to and the\n",
      "de: ich war das es zu sie er die der und\n",
      "nl: op te zijn dat hij van de het een en\n"
     ]
    }
   ],
   "source": [
    "# show most informative features\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "\n",
    "show_top10(text_clf.named_steps['clf'], text_clf.named_steps['vect'], data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which and how many features our model has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 754\n",
      "first features: ['aan' 'aber' 'about' 'after' 'again' 'ah' 'ai' 'air' 'al' 'all']\n",
      "last features: ['zu' 'zum' 'zwei' 'écria' 'étaient' 'était' 'été' 'één' 'être' 'über']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.asarray(text_clf.named_steps['vect'].get_feature_names())\n",
    "\n",
    "print('number of features: %d' % len(feature_names))\n",
    "print('first features: %s'% feature_names[0:10])\n",
    "print('last features: %s' % feature_names[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New data\n",
    "Let's try out the classifier with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo mein Name ist Hugo.  -->  de , prob: 0.84287621319\n",
      "Hi my name is Hugo.  -->  en , prob: 0.844883703028\n",
      "Bonjour mon nom est Hugo.  -->  fr , prob: 0.954010075416\n",
      "Hallo mijn naam is Hugo.  -->  nl , prob: 0.730366099539\n",
      "Eins, zwei und drei.  -->  de , prob: 0.924936109441\n",
      "One, two and three.  -->  en , prob: 0.976225623113\n",
      "Un, deux et trois.  -->  fr , prob: 0.989932321397\n",
      "Een, twee en drie.  -->  nl , prob: 0.962933421893\n"
     ]
    }
   ],
   "source": [
    "new_data = ['Hallo mein Name ist Hugo.', \n",
    "            'Hi my name is Hugo.', \n",
    "            'Bonjour mon nom est Hugo.',\n",
    "            'Hallo mijn naam is Hugo.',\n",
    "            'Eins, zwei und drei.',\n",
    "            'One, two and three.',\n",
    "            'Un, deux et trois.',\n",
    "            'Een, twee en drie.'\n",
    "           ]\n",
    "\n",
    "predicted = text_clf.predict(new_data)\n",
    "probs = text_clf.predict_proba(new_data)\n",
    "for i, p in enumerate(predicted):\n",
    "    print(new_data[i], ' --> ', data.target_names[p], ', prob:' , max(probs[i]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
