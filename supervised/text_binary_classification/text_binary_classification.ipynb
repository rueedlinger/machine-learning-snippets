{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Binary Classification (scikit-learn) with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this __Machine Learing Snippet__ we use scikit-learn (http://scikit-learn.org/) and ebooks from Project Gutenberg (https://www.gutenberg.org/) to create text binary classifier, which can classify German and English text.\n",
    "\n",
    "For our snippet we use the following ebooks:\n",
    "- Alice's Adventures in Wonderland by Lewis Carroll (English), https://www.gutenberg.org/ebooks/28885\n",
    "- Alice's Abenteuer im Wunderland by Lewis Carroll (German), https://www.gutenberg.org/ebooks/19778\n",
    "\n",
    "__Note:__\n",
    "The eBooks are for the use of anyone anywhere at no cost and with\n",
    "almost no restrictions whatsoever.  You may copy it, give it away or\n",
    "re-use it under the terms of the Project Gutenberg License included\n",
    "with this eBook or online at www.gutenberg.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Prepare the English and German text. Try to cut off the header and footer of the ebook. We use fixed values, this is not precise but will do the job.\n",
    "- cut off header / footer\n",
    "- convert to lowercase\n",
    "- tokenize (separated by space)\n",
    "- remove special chars\n",
    "- remove numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens (german) 24934\n",
      "tokens (english) 26678\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt_german = open('data/pg19778.txt', 'r').read()\n",
    "txt_english = open('data/pg28885.txt', 'r').read()\n",
    "\n",
    "feat_german = txt_german[5000: len(txt_german) - 20000].lower().strip().split()\n",
    "feat_english = txt_english[5000: len(txt_english) - 20000].lower().strip().split()\n",
    "\n",
    "def remove_special_chars(x):\n",
    "    \n",
    "    chars = ['_', '(', ')', '*', '\"', '[', ']', '?', '!', ',', '.', '»', '«', ':', ';']\n",
    "    for c in chars:\n",
    "        x = x.replace(c, '')\n",
    "    \n",
    "    # remove numbers\n",
    "    x = re.sub('\\d', '', x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "feat_english = [remove_special_chars(x) for x in feat_english]\n",
    "feat_german = [remove_special_chars(x) for x in feat_german]\n",
    "\n",
    "print('tokens (german)', len(feat_german))\n",
    "print('tokens (english)', len(feat_english))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Create text samples with 200 tokens (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples (german) 831\n",
      "samples (english) 889\n"
     ]
    }
   ],
   "source": [
    "def create_text_sample(x):\n",
    "    max_tokens = 30\n",
    "    data = []\n",
    "    text = []\n",
    "    for i, f in enumerate(x):\n",
    "        text.append(f)\n",
    "        if i % max_tokens == 0 and i != 0:\n",
    "            data.append(' '.join(text))\n",
    "            text = []\n",
    "    return data\n",
    "    \n",
    "\n",
    "sample_german = create_text_sample(feat_german)\n",
    "sample_english = create_text_sample(feat_english)\n",
    "\n",
    "print('samples (german)', len(sample_german))\n",
    "print('samples (english)', len(sample_english))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the text samples to train our binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sample:\n",
      "------------------\n",
      " an unusually large saucepan flew close by it and very nearly carried it off  it grunted again so violently that she looked down into its face in some alarm\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print('English sample:\\n------------------')\n",
    "print(sample_english[0])\n",
    "print('------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target names:  ['en', 'de']\n",
      "number of observations:  1720\n"
     ]
    }
   ],
   "source": [
    "import argparse as ap\n",
    "\n",
    "def create_sample(**kwargs):\n",
    "    samples = {'data': [], 'target': [], 'target_names':[]}\n",
    "    label = 0\n",
    "    for name, value in kwargs.items():\n",
    "        samples['target_names'].append(name)\n",
    "        for i in value:\n",
    "            samples['data'].append(i)\n",
    "            samples['target'].append(label)\n",
    "        label += 1\n",
    "            \n",
    "    \n",
    "    return ap.Namespace(**samples)\n",
    "\n",
    "data = create_sample(de = sample_german, en = sample_english)\n",
    "\n",
    "\n",
    "\n",
    "print('target names: ', data.target_names)\n",
    "print('number of observations: ', len(data.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def shuffle(x):\n",
    "    index = np.random.permutation(len(x.data))\n",
    "\n",
    "    X = np.array(x.data)[index]\n",
    "    y = np.array(x.target)[index]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = shuffle(data)\n",
    "\n",
    "folds = 4\n",
    "\n",
    "pipeline = Pipeline([('vect', TfidfVectorizer(analyzer='word', min_df=1, lowercase=True)),\n",
    "                      ('clf', MultinomialNB()),])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00 (+/- 0.00)\n",
      "[ 1.  1.  1.  1.]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         en       1.00      1.00      1.00       889\n",
      "         de       1.00      1.00      1.00       831\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(pipeline, X=X, y=y, cv=folds, scoring='f1_weighted')\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(scores)\n",
    "\n",
    "\n",
    "predicted = model_selection.cross_val_predict(pipeline, data.data, data.target, cv=folds)\n",
    "print(metrics.classification_report(data.target, predicted, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         en       1.00      1.00      1.00       188\n",
      "         de       1.00      1.00      1.00       156\n",
      "\n",
      "avg / total       1.00      1.00      1.00       344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=np.random.randint(low=0, high=10000))\n",
    "\n",
    "text_clf = pipeline.fit(x_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(x_test)\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    ('Die größte Macht hat das richtige Wort zur richtigen Zeit.', 0),\n",
    "    ('Leadership is the art of getting someone else to do something you want done because he wants to do it.', 1)\n",
    "            ]\n",
    "\n",
    "predicted = text_clf.predict([x[0] for x in new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
