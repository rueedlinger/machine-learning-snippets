{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Binary Classification (scikit-learn) with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this __Machine Learing Snippet__ we use scikit-learn (http://scikit-learn.org/) and ebooks from Project Gutenberg (https://www.gutenberg.org/) to create text binary classifier, which can classify German and English text.\n",
    "\n",
    "For our snippet we use the following ebooks:\n",
    "- Alice's Adventures in Wonderland by Lewis Carroll (English), https://www.gutenberg.org/ebooks/28885\n",
    "- Alice's Abenteuer im Wunderland by Lewis Carroll (German), https://www.gutenberg.org/ebooks/19778\n",
    "\n",
    "__Note:__\n",
    "The eBooks are for the use of anyone anywhere at no cost and with\n",
    "almost no restrictions whatsoever.  You may copy it, give it away or\n",
    "re-use it under the terms of the Project Gutenberg License included\n",
    "with this eBook or online at www.gutenberg.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Prepare the English and German text. Try to cut off the header and footer of the ebook. We use fixed values, this is not precise but will do the job.\n",
    "- cut off header / footer\n",
    "- convert to lowercase\n",
    "- tokenize (separated by space)\n",
    "- remove special chars\n",
    "- remove numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens (german) 24934\n",
      "tokens (english) 26678\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt_german = open('data/pg19778.txt', 'r').read()\n",
    "txt_english = open('data/pg28885.txt', 'r').read()\n",
    "\n",
    "feat_german = txt_german[5000: len(txt_german) - 20000].lower().strip().split()\n",
    "feat_english = txt_english[5000: len(txt_english) - 20000].lower().strip().split()\n",
    "\n",
    "def remove_special_chars(x):\n",
    "    \n",
    "    chars = ['_', '(', ')', '*', '\"', '[', ']', '?', '!', ',', '.', '»', '«', ':', ';']\n",
    "    for c in chars:\n",
    "        x = x.replace(c, '')\n",
    "    \n",
    "    # remove numbers\n",
    "    x = re.sub('\\d', '', x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "feat_english = [remove_special_chars(x) for x in feat_english]\n",
    "feat_german = [remove_special_chars(x) for x in feat_german]\n",
    "\n",
    "print('tokens (german)', len(feat_german))\n",
    "print('tokens (english)', len(feat_english))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Create text samples with 200 tokens (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples (german) 1246\n",
      "samples (english) 1333\n"
     ]
    }
   ],
   "source": [
    "def create_text_sample(x):\n",
    "    max_tokens = 20\n",
    "    data = []\n",
    "    text = []\n",
    "    for i, f in enumerate(x):\n",
    "        text.append(f)\n",
    "        if i % max_tokens == 0 and i != 0:\n",
    "            data.append(' '.join(text))\n",
    "            text = []\n",
    "    return data\n",
    "    \n",
    "\n",
    "sample_german = create_text_sample(feat_german)\n",
    "sample_english = create_text_sample(feat_english)\n",
    "\n",
    "print('samples (german)', len(sample_german))\n",
    "print('samples (english)', len(sample_english))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the text samples to train our binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sample:\n",
      "------------------\n",
      " an unusually large saucepan flew close by it and very nearly carried it off  it grunted again so violently\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print('English sample:\\n------------------')\n",
    "print(sample_english[0])\n",
    "print('------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "- Create the data structure\n",
    "- Split test and training set\n",
    "- Create the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target names:  ['en', 'de']\n",
      "number of observations:  2579\n"
     ]
    }
   ],
   "source": [
    "import argparse as ap\n",
    "\n",
    "def create_data_structure(**kwargs):\n",
    "    samples = {'data': [], 'target': [], 'target_names':[]}\n",
    "    label = 0\n",
    "    for name, value in kwargs.items():\n",
    "        samples['target_names'].append(name)\n",
    "        for i in value:\n",
    "            samples['data'].append(i)\n",
    "            samples['target'].append(label)\n",
    "        label += 1\n",
    "            \n",
    "    \n",
    "    return ap.Namespace(**samples)\n",
    "\n",
    "data = create_data_structure(de = sample_german, en = sample_english)\n",
    "\n",
    "\n",
    "\n",
    "print('target names: ', data.target_names)\n",
    "print('number of observations: ', len(data.data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (x, y):  2063 2063\n",
      "test size (x, y):  516 516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.20, random_state=np.random.randint(low=0, high=10000))\n",
    "\n",
    "print('train size (x, y): ', len(x_train),  len(y_train))\n",
    "print('test size (x, y): ', len(x_test), len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Machine Learning Pipeline (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "pipeline = Pipeline([('vect', TfidfVectorizer(analyzer='word', min_df=1, lowercase=True)),\n",
    "                      ('clf', MultinomialNB()),])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "- Evaluate the model with k-fold with the training set\n",
    "- Evaluate the model with the holdout set (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00 (+/- 0.00)\n",
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "folds = 5\n",
    "scores = model_selection.cross_val_score(pipeline, X=x_train, y=y_train, cv=folds, scoring='accuracy')\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         en       1.00      1.00      1.00      1061\n",
      "         de       1.00      1.00      1.00      1002\n",
      "\n",
      "avg / total       1.00      1.00      1.00      2063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model_selection.cross_val_predict(pipeline, X=x_train, y=y_train, cv=folds)\n",
    "print(metrics.classification_report(y_train, predicted, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         en       1.00      1.00      1.00       272\n",
      "         de       1.00      1.00      1.00       244\n",
      "\n",
      "avg / total       1.00      1.00      1.00       516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = pipeline.fit(x_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(x_test)\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted, target_names=data.target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
